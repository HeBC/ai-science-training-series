{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa334eb6-2898-4ee9-b777-673b218574f4",
   "metadata": {},
   "source": [
    "# My homework  -- Session 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82391bf5-c829-426f-8269-063d91d72ddf",
   "metadata": {},
   "source": [
    "Homework 1:\r\n",
    "In this notebook, we've learned about some basic convolutional networks and trained one on CIFAR-10 images. It did ... OK. There is significant overfitting of this model. There are some ways to address that, but we didn't have time to get into that in this session.\r\n",
    "\r\n",
    "Meanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture. Change one or more of the following:\r\n",
    "\r\n",
    "The number of convolutions between downsampling\r\n",
    "The number of filters in each layer\r\n",
    "The initial \"patchify\" layer\r\n",
    "Another hyper-parameter of your choosing\r\n",
    "And compare your final validation accuracy to the accuracy shown here. Can you beat the validation accuracy shown?\r\n",
    "\r\n",
    "For full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets' performance (loss and accuracy) for all the epochs you train. You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37511a7d-7020-4dbb-85d5-af25ec3eb53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"datasets/CIFAR-10/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=32, scale=[0.85,1.0], antialias=False),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"datasets/CIFAR-10/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc9de39-69f2-45b8-88c1-70fe104beef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=True, \n",
    "    num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=False, \n",
    "    num_workers=4)\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shape, stride=2):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size = stride,\n",
    "            stride = stride,\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "\n",
    "        return self.downsample(self.norm(inputs))\n",
    "        \n",
    "        \n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"This block of operations is loosely based on this paper:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, shape):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=in_channels, \n",
    "                                     groups=in_channels,\n",
    "                                     kernel_size=[7,7],\n",
    "                                     padding='same' )\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=4*in_channels,\n",
    "                                     kernel_size=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n",
    "                                     out_channels=in_channels,\n",
    "                                     kernel_size=1\n",
    "                                     )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # This is a downsampling convolution that will produce patches of output.\n",
    "\n",
    "        # This is similar to what vision transformers do to tokenize the images.\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                                    out_channels=n_initial_filters,\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1)\n",
    "        \n",
    "        current_shape = [32, 32]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([n_initial_filters,*current_shape])\n",
    "        # self.norm1 = WrappedLayerNorm()\n",
    "\n",
    "        current_n_filters = n_initial_filters\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        for i, n_blocks in enumerate(range(n_stages)):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n",
    "            # Add a downsampling layer:\n",
    "            if i != n_stages - 1:\n",
    "                # Skip downsampling if it's the last layer!\n",
    "                self.layers.append(Downsampler(\n",
    "                    in_channels=current_n_filters, \n",
    "                    out_channels=2*current_n_filters,\n",
    "                    shape = current_shape,\n",
    "                    )\n",
    "                )\n",
    "                # Double the number of filters:\n",
    "                current_n_filters = 2*current_n_filters\n",
    "                # Cut the shape in half:\n",
    "                current_shape = [ cs // 2 for cs in current_shape]\n",
    "            \n",
    "\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(current_n_filters),\n",
    "            nn.Linear(current_n_filters, 10)\n",
    "        )\n",
    "        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n",
    "        # # This brings it down to one channel / class\n",
    "        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, \n",
    "        #                                   kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.stem(inputs)\n",
    "        # Apply a normalization after the initial patching:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Apply the main chunk of the network:\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Normalize and readout:\n",
    "        x = nn.functional.avg_pool2d(x, x.shape[2:])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            val_bar.update()\n",
    "            \n",
    "    loss /= num_batches\n",
    "    correct /= (size*batch_size)\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()      \n",
    "\n",
    "        progress_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5c76d2-06b2-4459-8775-53cfc9c90792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bee9d63bc1d472b9feebf6cb552e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_bar:\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# checking on the training loss and accuracy once per epoch\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidate (train) Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_eval:\n",
      "Cell \u001b[0;32mIn[2], line 185\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dataloader, model, loss_fn, optimizer, progress_bar)\u001b[0m\n\u001b[1;32m    182\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# backward pass calculates gradients\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# take one step with these gradients\u001b[39;00m\n\u001b[1;32m    188\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Classifier(64, 4, 2)\n",
    "\n",
    "#model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "for j in range(epochs):\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
    "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "        \n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
    "        acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
    "\n",
    "        print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
    "    with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "    \n",
    "        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n",
    "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6d986-8a9a-47c1-9f78-759fd9761435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
